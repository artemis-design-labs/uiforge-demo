# LLM Provider Configuration
# Choose 'local' for local models or 'anthropic' for Claude API
LLM_PROVIDER=local

# Local LLM Configuration (when LLM_PROVIDER=local)
# Backend options: ollama, llamacpp, transformers, custom
LLM_BACKEND=ollama
LLM_MODEL=codellama:7b
LLM_API_URL=http://localhost:11434

# Anthropic Configuration (when LLM_PROVIDER=anthropic)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI Configuration (future support)
# OPENAI_API_KEY=your_openai_api_key_here

# Service Configuration
CODEGEN_PORT=8081
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080

# NPM Publishing (optional for local testing)
NPM_TOKEN=your_npm_token_here
NPM_REGISTRY=https://registry.npmjs.org

# Default package scope (can be overridden per request)
DEFAULT_PACKAGE_SCOPE=@uiforge